blogdown::stop_server()
blogdown::serve_site()
blogdown:::preview_site(startup = TRUE)
blogdown::serve_site()
blogdown::serve_site()
data <- matrix(rnorm(1e5), nrows = 10000)
data <- matrix(rnorm(1e5), nrow = 10000)
dim(data)
data <- matrix(rnorm(1e5), nrow = 10000)
# Splitting data into train and test
set.seed(101)
sample <- sample.int(n = nrow(data), size = floor(.75 * nrow(data)), replace = F)
train <- as.data.table(data[sample, ])
library(data.table)
train <- as.data.table(data[sample, ])
test <- as.data.table(data[-sample, ])
# Preparing the training set
cols <- c(names(train))
train %<>% mutate_at(cols, as.factor)
??%<>%
??`%<>%`
library(magrittr)
# Preparing the training set
cols <- c(names(train))
train %<>% mutate_at(cols, as.factor)
library(tidyverse)
# Splitting data into train and test
set.seed(101)
sample <- sample.int(n = nrow(data), size = floor(.75 * nrow(data)), replace = F)
train <- as.data.table(data[sample, ])
test <- as.data.table(data[-sample, ])
# Preparing the training set
cols <- c(names(train))
train %<>% mutate_at(cols, as.factor)
train %<>% mutate_at(cols, as.numeric)
train %<>% mutate_at(cols,~(scale(.) %>% as.vector))
train[is.na(train) ] <- 0
train
library(keras)
# Splitting train into x (parameters) and y (prediction values)
x_train <- data.matrix(train[,-"1"])
# Splitting train into x (parameters) and y (prediction values)
x_train <- data.matrix(train[,-"V1"])
y_train <- data.matrix(train[, "V1"])
x_train
dim(x_train)
dim(y_train)
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = dim(x_train)[-1]) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = length(unique(data$`1`)), activation = 'softmax')
dim(x_train)[-1]
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = list(dim(x_train)[-1])) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = length(unique(data$`1`)), activation = 'softmax')
data <- matrix(rnorm(1e5), nrow = 10000) %>% as.tibble()
data <- matrix(rnorm(1e5), nrow = 10000) %>% as_tibble()
glimpse(data)
"a":"b"
data$y <- sample(c("a", "b"), nrow(data), replace =T)
data <- matrix(rnorm(1e5), nrow = 10000) %>% as_tibble()
data$y <- sample(c("a", "b"), nrow(data), replace =T)
# Splitting data into train and test
set.seed(101)
sample <- sample.int(n = nrow(data), size = floor(.75 * nrow(data)), replace = F)
train <- as.data.table(data[sample, ])
test <- as.data.table(data[-sample, ])
# Preparing the training set
cols <- c(names(train))
train %<>% mutate_at(cols, as.factor)
train %<>% mutate_at(cols, as.numeric)
train %<>% mutate_at(cols,~(scale(.) %>% as.vector))
train[is.na(train) ] <- 0
# Splitting train into x (parameters) and y (prediction values)
x_train <- data.matrix(train[,-"y"])
y_train <- data.matrix(train[, "y"])
glimpse(x_train)
head(x_train)
head(y_train)
# Splitting train into x (parameters) and y (prediction values)
x_train <- data.matrix(train[,-"y"])
y_train <- data.matrix(train[, "y"])
dim(x_train)
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = dim(x_train)[-1]) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = length(unique(data$`y`)), activation = 'softmax')
# Summarize model layers and units
summary(model)
# Compile model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 32,
validation_split = 0.2
)
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = list(NULL, 9)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = length(unique(data$`y`)), activation = 'softmax')
# Summarize model layers and units
summary(model)
# Compile model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 32,
validation_split = 0.2
)
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = list(9, 1)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = length(unique(data$`y`)), activation = 'softmax')
# Summarize model layers and units
summary(model)
# Compile model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 32,
validation_split = 0.2
)
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh') %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = length(unique(data$`y`)), activation = 'softmax')
# Summarize model layers and units
summary(model)
head(x_train)
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = dim(1, 10)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = length(unique(data$`y`)), activation = 'softmax')
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = list(1, 10)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = length(unique(data$`y`)), activation = 'softmax')
# Summarize model layers and units
summary(model)
# Compile model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 32,
validation_split = 0.2
)
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = list(1, 10, NULL)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = length(unique(data$`y`)), activation = 'softmax')
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = list(1, 10, NULL)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = length(unique(data$`y`)), activation = 'softmax')
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = list(1, 10)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = length(unique(data$`y`)), activation = 'softmax')
# Summarize model layers and units
summary(model)
# Compile model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 32,
validation_split = 0.2
)
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = list(10, 1)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = length(unique(data$`y`)), activation = 'softmax')
# Summarize model layers and units
summary(model)
# Compile model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 32,
validation_split = 0.2
)
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = 10) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = length(unique(data$`y`)), activation = 'softmax')
# Summarize model layers and units
summary(model)
# Compile model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 32,
validation_split = 0.2
)
length(unique(data$`y`)
)
y_train
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = 10) %>%
layer_dense(units = 1, activation = 'sigmoid')
# Summarize model layers and units
summary(model)
# Compile model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 32,
validation_split = 0.2
)
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = 10) %>%
layer_dense(units = 2, activation = 'sigmoid')
# Summarize model layers and units
summary(model)
# Compile model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 32,
validation_split = 0.2
)
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = 10) %>%
layer_dense(units = 1, activation = 'sigmoid')
# Summarize model layers and units
summary(model)
# Compile model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 32,
validation_split = 0.2
)
length(unique(data$`1`)
length(unique(data$`1`))
length(unique(data$`y`))
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = 10) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = 2, activation = 'softmax')
# Summarize model layers and units
summary(model)
# Compile model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 32,
validation_split = 0.2
)
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = 10) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = 1, activation = 'softmax')
# Summarize model layers and units
summary(model)
# Compile model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 32,
validation_split = 0.2
)
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = 10) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = 2, activation = 'softmax')
# Summarize model layers and units
summary(model)
# Compile model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 32,
validation_split = 0.2
)
# Designing model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'tanh', input_shape = 10) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = 1, activation = 'softmax')
# Summarize model layers and units
summary(model)
# Compile model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = 100, batch_size = 32,
validation_split = 0.2
)
x <- 1:12; n <- 2
a = round((min(x)/10),0)*10
a
b = round((max(x)/10),0)*10
b
c = round((b-a)/n,0)
c
z = seq(a,b,c)
z
z = ifelse(max(z)<b,append(z,max(z)+c),z)
z
a
b
install.packages("blogdown")
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
install.packages("blogdown")
blogdown::serve_site()
blogdown::install_hugo()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::hugo_available()
blogdown::hugo_version()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
